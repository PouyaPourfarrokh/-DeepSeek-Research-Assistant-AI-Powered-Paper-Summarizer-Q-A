{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† DeepSeek Research Assistant üìÑüîç  \n",
    "## **AI-Powered Research Paper Q&A & Summarization**  \n",
    "\n",
    "üîç **Analyze any research paper with AI!**  \n",
    "‚úÖ **Upload a PDF**  \n",
    "‚úÖ **Get AI-generated summaries**  \n",
    "‚úÖ **Ask any questions, and get answers with citations**  \n",
    "‚úÖ **Retrieve key insights instantly**  \n",
    "\n",
    "---\n",
    "\n",
    "### **‚öôÔ∏è Tech Stack**\n",
    "üîπ **LLM Engine**: DeepSeek-R1-8B (via Ollama)  \n",
    "üîπ **AI Framework**: LangChain (retrieval & prompt engineering)  \n",
    "üîπ **Text Extraction**: pdfminer.six  \n",
    "üîπ **Semantic Search**: ChromaDB (BM25 + Embeddings)  \n",
    "\n",
    "---\n",
    "\n",
    "### **üöÄ How to Use**\n",
    "1Ô∏è‚É£ **Run all cells**  \n",
    "2Ô∏è‚É£ **Place a research paper (PDF) inside `Research_papers/`**  \n",
    "3Ô∏è‚É£ **Ask any question**, and the AI retrieves & answers  \n",
    "4Ô∏è‚É£ **All responses are logged for reference**  \n",
    "\n",
    "---\n",
    "\n",
    "üë®‚Äçüíª **Made with ‚ù§Ô∏è using DeepSeek-R1 & LangChain**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 2: Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All Packages Imported Successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import pdfminer.high_level\n",
    "import streamlit as st  # Optional for UI\n",
    "import time\n",
    "\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Embeddings & Vector Storage\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "# Retrieval Enhancements\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import BM25Retriever, ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.document_transformers import LongContextReorder\n",
    "\n",
    "print(\"‚úÖ All Packages Imported Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 3: Extract Text from PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted text from: DeepSeek_V3.pdf\n",
      "DeepSeek-V3 Technical Report\n",
      "\n",
      "DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full \n"
     ]
    }
   ],
   "source": [
    "pdf_directory = \"/Users/pouyapourfarrokh/Desktop/AI&Data science Projects/DeepSeek Research Assistant/-DeepSeek-Research-Assistant-AI-Powered-Paper-Summarizer-Q-A/Research_papers\"\n",
    "\n",
    "def get_latest_pdf(directory):\n",
    "    \"\"\"Retrieve the most recently added PDF file from the directory.\"\"\"\n",
    "    pdf_files = glob.glob(os.path.join(directory, \"*.pdf\"))\n",
    "    return max(pdf_files, key=os.path.getctime) if pdf_files else None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a given PDF file.\"\"\"\n",
    "    if not pdf_path or not os.path.exists(pdf_path):\n",
    "        return None\n",
    "    try:\n",
    "        text = pdfminer.high_level.extract_text(pdf_path)\n",
    "        return text if text.strip() else \"‚ö†Ô∏è No extractable text found.\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error extracting text: {str(e)}\"\n",
    "\n",
    "latest_pdf = get_latest_pdf(pdf_directory)\n",
    "\n",
    "if latest_pdf:\n",
    "    extracted_text = extract_text_from_pdf(latest_pdf)\n",
    "    print(f\"‚úÖ Extracted text from: {os.path.basename(latest_pdf)}\")\n",
    "    print(extracted_text[:1000])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No PDFs found in the directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 4: Chunk the Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully split the document into 97 chunks.\n"
     ]
    }
   ],
   "source": [
    "if extracted_text and \"‚ùå Error\" not in extracted_text and \"‚ö†Ô∏è No extractable text\" not in extracted_text:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "    text_chunks = text_splitter.split_text(extracted_text)\n",
    "    print(f\"‚úÖ Successfully split the document into {len(text_chunks)} chunks.\")\n",
    "else:\n",
    "    text_chunks = []\n",
    "    print(\"‚ö†Ô∏è No valid text found for splitting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 5: Store chunks in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indexed 97 chunks in ChromaDB at /Users/pouyapourfarrokh/Desktop/AI&Data science Projects/DeepSeek Research Assistant/-DeepSeek-Research-Assistant-AI-Powered-Paper-Summarizer-Q-A/db/chroma_db (Dimension: 384)\n"
     ]
    }
   ],
   "source": [
    "chroma_db_path = \"/Users/pouyapourfarrokh/Desktop/AI&Data science Projects/DeepSeek Research Assistant/-DeepSeek-Research-Assistant-AI-Powered-Paper-Summarizer-Q-A/db/chroma_db\"\n",
    "\n",
    "if not text_chunks:\n",
    "    print(\"‚ö†Ô∏è No valid text chunks found. Skipping vector storage.\")\n",
    "else:\n",
    "    embedding_model = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "    vector_db = Chroma.from_texts(text_chunks, embedding=embedding_model, persist_directory=chroma_db_path)\n",
    "    print(f\"‚úÖ Indexed {len(text_chunks)} chunks in ChromaDB at {chroma_db_path} (Dimension: 384)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_text(query, document_chunks, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant text segments for answering a query.\n",
    "    \"\"\"\n",
    "    relevant_text = []\n",
    "    query_keywords = set(query.lower().split())\n",
    "\n",
    "    for section in document_chunks:\n",
    "        section_words = set(section.lower().split())\n",
    "        overlap = len(query_keywords.intersection(section_words))\n",
    "\n",
    "        if overlap > 0:\n",
    "            relevant_text.append(section)\n",
    "\n",
    "    return \" \".join(relevant_text[:top_k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=\"\"\"\n",
    "You are an AI research assistant. Answer the user's question based **only on the provided research paper content**.\n",
    "\n",
    "### **Strict Instructions:**  \n",
    "- Do **NOT** include speculation, `<think>`, or information beyond the document.  \n",
    "- Your response **MUST** follow a structured format.  \n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Answer**  \n",
    "\n",
    "### **1Ô∏è‚É£ Key Insights**  \n",
    "- <Summarize the most important information related to the user's question.>  \n",
    "\n",
    "### **2Ô∏è‚É£ Supporting Evidence**  \n",
    "- <Provide details, data, or results from the research paper to support the answer.>  \n",
    "\n",
    "### **3Ô∏è‚É£ Implications or Applications**  \n",
    "- <Explain what this means in a broader scientific or technical context. If not applicable, state \"Not applicable.\">  \n",
    "\n",
    "---\n",
    "\n",
    "### **üìñ Source:**  \n",
    "<Cite the section, table, or figure from the retrieved research paper. If no citation is found, state \"Source not explicitly provided.\">  \n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Q&A Logging Enabled. Type 'end' to exit anytime.\n",
      "\u001b[H\u001b[2J"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üëã Exiting Q&A mode. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "log_file_path = \"qna_log.json\"\n",
    "\n",
    "def save_response_to_log(question, response):\n",
    "    \"\"\"Saves each question and answer to a JSON log file.\"\"\"\n",
    "    try:\n",
    "        # Load existing log file if available\n",
    "        if os.path.exists(log_file_path):\n",
    "            with open(log_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                log_data = json.load(f)\n",
    "        else:\n",
    "            log_data = []\n",
    "\n",
    "        # Append the new question-response pair\n",
    "        log_data.append({\"question\": question, \"response\": response})\n",
    "\n",
    "        # Save back to the log file\n",
    "        with open(log_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(log_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error logging response: {str(e)}\")\n",
    "\n",
    "def clear_console():\n",
    "    \"\"\"Clears the console before displaying the next response.\"\"\"\n",
    "    os.system('cls' if os.name == 'nt' else 'clear')\n",
    "\n",
    "def interactive_qa():\n",
    "    \"\"\"Handles the interactive Q&A loop for any research paper.\"\"\"\n",
    "    if not retriever:\n",
    "        print(\"\\n‚ö†Ô∏è No valid retriever found. Please check the document processing pipeline.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        # ‚úÖ Clear previous response BEFORE showing the next input prompt\n",
    "        clear_console()\n",
    "\n",
    "        user_question = input(\"\\n‚ùì **Your Question (type 'end' to exit):** \").strip()\n",
    "\n",
    "        if user_question.lower() == \"end\":\n",
    "            print(\"\\nüëã Exiting Q&A mode. Have a great day!\")\n",
    "            break\n",
    "\n",
    "        if not user_question:\n",
    "            print(\"\\n‚ö†Ô∏è Please enter a valid question.\")\n",
    "            continue\n",
    "        \n",
    "        best_context = retrieve_relevant_text(user_question, text_chunks, top_k=3)\n",
    "\n",
    "        if not best_context:\n",
    "            print(\"\\n‚ö†Ô∏è No relevant information found. Try rephrasing the question.\")\n",
    "            continue\n",
    "\n",
    "        raw_response = qa_chain.invoke({\"question\": user_question, \"context\": best_context})\n",
    "        structured_answer = process_and_display_response(raw_response)\n",
    "\n",
    "        # ‚úÖ Save response to JSON file\n",
    "        save_response_to_log(user_question, structured_answer)\n",
    "\n",
    "        # ‚úÖ Display the latest structured response\n",
    "        print(\"\\nüìå **Final Answer:**\\n\", structured_answer)\n",
    "\n",
    "        # ‚úÖ Small delay before allowing the next question\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"‚úÖ Q&A Logging Enabled. Type 'end' to exit anytime.\")\n",
    "\n",
    "interactive_qa()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
