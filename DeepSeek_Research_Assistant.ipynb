{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† DeepSeek Research Assistant üìÑüîç  \n",
    "### AI-Powered Research Paper Summarization & Q&A using DeepSeek-R1 & LangChain  \n",
    "This project allows **students & professors** to:  \n",
    "‚úÖ Upload a **research paper (PDF)**  \n",
    "‚úÖ Get an **AI-generated summary**  \n",
    "‚úÖ Receive **suggested questions** for better understanding  \n",
    "‚úÖ **Ask custom questions** for deeper insights  \n",
    "\n",
    "### ‚öôÔ∏è Tech Stack:  \n",
    "- **DeepSeek-R1-8B** (via Ollama) ‚Äì AI-powered text analysis  \n",
    "- **LangChain** ‚Äì Prompt engineering & AI interaction  \n",
    "- **ChromaDB** ‚Äì Vector database for semantic search  \n",
    "- **pdfminer.six** ‚Äì Extract text from PDFs  \n",
    "- **Streamlit** ‚Äì User-friendly UI (for deployment)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 2: Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Packages Imported Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4_/sz_9qvkn7cj9h353z48p55dw0000gn/T/ipykernel_62380/4015808447.py:11: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"huihui_ai/deepseek-r1-abliterated:8b\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pdfminer.high_level\n",
    "import langchain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import streamlit as st\n",
    "\n",
    "# Initialize the DeepSeek model\n",
    "llm = Ollama(model=\"huihui_ai/deepseek-r1-abliterated:8b\")\n",
    "\n",
    "print(\"‚úÖ Packages Imported Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 3: Extract Text from PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted text from: /Users/pouyapourfarrokh/Desktop/AI&Data science Projects/DeepSeek Research Assistant/-DeepSeek-Research-Assistant-AI-Powered-Paper-Summarizer-Q-A/Research_papers/DeepSeek_V3.pdf\n",
      "DeepSeek-V3 Technical Report\n",
      "\n",
      "DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full \n"
     ]
    }
   ],
   "source": [
    "pdf_directory = \"/Users/pouyapourfarrokh/Desktop/AI&Data science Projects/DeepSeek Research Assistant/-DeepSeek-Research-Assistant-AI-Powered-Paper-Summarizer-Q-A/Research_papers\"\n",
    "\n",
    "def get_latest_pdf(directory):\n",
    "    pdf_files = sorted(glob.glob(os.path.join(directory, \"*.pdf\")), key=os.path.getctime, reverse=True)\n",
    "    return pdf_files[0] if pdf_files else None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return pdfminer.high_level.extract_text(pdf_path)\n",
    "\n",
    "latest_pdf = get_latest_pdf(pdf_directory)\n",
    "\n",
    "if latest_pdf:\n",
    "    extracted_text = extract_text_from_pdf(latest_pdf)\n",
    "    print(f\"‚úÖ Extracted text from: {latest_pdf}\")\n",
    "    print(extracted_text[:1000])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No PDFs found in the directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 4: Integrating LangChain for Summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4_/sz_9qvkn7cj9h353z48p55dw0000gn/T/ipykernel_62380/3182845835.py:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
      "/var/folders/4_/sz_9qvkn7cj9h353z48p55dw0000gn/T/ipykernel_62380/3182845835.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return summary_chain.run(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Research Paper Summary:\n",
      " <think>\n",
      "Okay, so I need to help summarize the research paper text for DeepSeek-V3 in clear and concise points. Let me read through the provided content carefully.\n",
      "\n",
      "The abstract mentions that DeepSeek-V3 is a strong Mixture-of-Experts (MoE) language model with 671B total parameters, specifically 37B activated per token. It uses Multi-head Latent Attention (MLA) and DeepSeekMoE architectures validated in DeepSeek-V2. They've introduced an auxiliary-loss-free strategy for load balancing and set a multi-token prediction training objective for better performance. The model is pre-trained on 14.8 trillion tokens, then goes through Supervised Fine-Tuning and Reinforcement Learning stages. Evaluations show it outperforms open-source models and matches closed-source ones, all while using only about 2.788M H800 GPU hours for training and being very stable without loss spikes.\n",
      "\n",
      "Looking at the sections: Introduction talks about the model's strengths; Architecture goes into details about MLA and their MoE structure with load balancing; Infrastructure covers compute clusters, training framework (DualPipe and Computation-Communication Overlap), FP8 training with mixed precision and quantization; Inference and Deployment mentions efficiency in inference.\n",
      "\n",
      "So I need to condense all this into key points without too much jargon. The user wants a clear summary, so focusing on the main innovations, performance highlights, training efficiency, architecture features, and evaluation results would be essential.\n",
      "</think>\n",
      "\n",
      "**Summary of DeepSeek-V3 Technical Report**\n",
      "\n",
      "1. **Model Overview**:  \n",
      "   - **Type**: Strong Mixture-of-Experts (MoE) language model.  \n",
      "   - **Parameters**: 671B total parameters, with 37B activated per token.  \n",
      "   - **Architecture Features**: Utilizes Multi-head Latent Attention (MLA) and DeepSeekMoE architecture, which includes an auxiliary-loss-free load balancing strategy and multi-token prediction training objective for improved performance.\n",
      "\n",
      "2. **Training Process**:  \n",
      "   - **Pre-training**: Pre-trained on 14.8 trillion diverse and high-quality tokens.  \n",
      "   - **Fine-tuning & Reinforcement Learning (RL)**: Followed by Supervised Fine-Tuning and RL stages to enhance capabilities.\n",
      "\n",
      "3. **Performance and Evaluation**:  \n",
      "   - **Comparisons**: Outperforms open-source models and matches leading closed-source models in performance.  \n",
      "   - **Training Efficiency**: Requires only 2.788M H800 GPU hours for full training, with a stable process that avoids loss spikes or rollbacks.  \n",
      "\n",
      "4. **Innovation and Features**:  \n",
      "   - **Load Balancing Strategy**: Auxiliary-loss-free method for efficient load balancing.  \n",
      "   - **Training Objective**: Multi-token prediction objective to enhance model performance.  \n",
      "\n",
      "5. **Infrastructure and Implementation**:  \n",
      "   - **Compute Clusters and Training Framework**: Uses DualPipe with computation-communication overlap for efficiency, and employs FP8 training with mixed precision, quantization, and low-precision storage to optimize performance.\n",
      "\n",
      "6. **Inference and Deployment**:  \n",
      "   - The model is designed for efficient inference and deployment, with checkpoints available on GitHub.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LangChain with DeepSeek model\n",
    "llm = Ollama(model=\"huihui_ai/deepseek-r1-abliterated:8b\")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"Summarize the following research paper text in clear and concise points:\\n\\n{text}\\n\\n### Summary:\"\"\"\n",
    ")\n",
    "\n",
    "# LangChain Summarization Chain\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
    "\n",
    "def summarize_text(text):\n",
    "    text = text[:3000]  # Limit to model's token capacity\n",
    "    return summary_chain.run(text)\n",
    "\n",
    "# Generate summary\n",
    "summary = summarize_text(extracted_text)\n",
    "print(\"üìå Research Paper Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 5: LangChain for Generating Suggested Questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Suggested Questions:\n",
      " <think>\n",
      "\n",
      "1. How does the auxiliary-loss-free load balancing strategy in DeepSeek-V3 enhance the training efficiency compared to traditional methods?\n",
      "\n",
      "2. What specific improvements in performance are attributed to the multi-token prediction training objective in DeepSeek-V3? Can you provide examples of these improvements?\n",
      "\n",
      "3. What are the key differences between the architecture of DeepSeek-V3 and its predecessor, DeepSeek-V2, and how do these contribute to better performance?\n",
      "\n",
      "4. How does the use of mixed precision (FP8) and quantization techniques in training affect the model's final performance and computational requirements? Are there trade-offs between precision and speed that are worth considering?\n",
      "\n",
      "5. Given the significant reduction in GPU hours required for training, what implications does this have for deploying DeepSeek-V3 in resource-constrained environments? How can the efficiency be further optimized?\n"
     ]
    }
   ],
   "source": [
    "question_prompt = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=\"\"\"Based on the following research paper summary, generate 5 thought-provoking questions to help understand the topic better:\\n\\n{summary}\\n\\n### Questions:\"\"\"\n",
    ")\n",
    "\n",
    "question_chain = LLMChain(llm=llm, prompt=question_prompt)\n",
    "\n",
    "def generate_questions(summary):\n",
    "    return question_chain.run(summary)\n",
    "\n",
    "# Generate suggested questions\n",
    "suggested_questions = generate_questions(summary)\n",
    "\n",
    "print(\"üìå Suggested Questions:\\n\", suggested_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
