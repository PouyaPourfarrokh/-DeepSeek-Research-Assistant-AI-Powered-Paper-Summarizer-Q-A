{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† DeepSeek Research Assistant üìÑüîç  \n",
    "### AI-Powered Research Paper Summarization & Q&A using DeepSeek-R1 & LangChain  \n",
    "\n",
    "This project allows **students & professors** to:  \n",
    "‚úÖ Upload a **research paper (PDF)**  \n",
    "‚úÖ Get an **AI-generated summary**  \n",
    "‚úÖ Receive **suggested questions** for better understanding  \n",
    "‚úÖ **Ask custom questions**, and the model searches the ENTIRE paper before answering  \n",
    "\n",
    "### ‚öôÔ∏è Tech Stack:  \n",
    "- **DeepSeek-R1-8B** (via Ollama) ‚Äì AI-powered text analysis  \n",
    "- **LangChain** ‚Äì Retrieval-based Q&A & prompt engineering  \n",
    "- **ChromaDB** ‚Äì Vector database for semantic search  \n",
    "- **pdfminer.six** ‚Äì Extract text from PDFs  \n",
    "- **Streamlit** ‚Äì User-friendly UI (for deployment)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 2: Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pdfminer.high_level\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "import streamlit as st  # For future UI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 3: Extract Text from PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted text from: /Users/pouyapourfarrokh/Desktop/AI&Data science Projects/DeepSeek Research Assistant/-DeepSeek-Research-Assistant-AI-Powered-Paper-Summarizer-Q-A/Research_papers/DeepSeek_V3.pdf\n",
      "DeepSeek-V3 Technical Report\n",
      "\n",
      "DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full \n"
     ]
    }
   ],
   "source": [
    "pdf_directory = \"/Users/pouyapourfarrokh/Desktop/AI&Data science Projects/DeepSeek Research Assistant/-DeepSeek-Research-Assistant-AI-Powered-Paper-Summarizer-Q-A/Research_papers\"\n",
    "\n",
    "def get_latest_pdf(directory):\n",
    "    \"\"\"Retrieve the latest added PDF file from the directory.\"\"\"\n",
    "    pdf_files = sorted(glob.glob(os.path.join(directory, \"*.pdf\")), key=os.path.getctime, reverse=True)\n",
    "    return pdf_files[0] if pdf_files else None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a given PDF file.\"\"\"\n",
    "    return pdfminer.high_level.extract_text(pdf_path)\n",
    "\n",
    "# Get latest uploaded PDF\n",
    "latest_pdf = get_latest_pdf(pdf_directory)\n",
    "\n",
    "if latest_pdf:\n",
    "    extracted_text = extract_text_from_pdf(latest_pdf)\n",
    "    print(f\"‚úÖ Extracted text from: {latest_pdf}\")\n",
    "    print(extracted_text[:1000])  # Preview first 1000 characters\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No PDFs found in the directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 4: Chunk the Text & Store in ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indexed 216 chunks in ChromaDB for document search.\n"
     ]
    }
   ],
   "source": [
    "# Initialize text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "# Split research paper into chunks\n",
    "text_chunks = text_splitter.split_text(extracted_text)\n",
    "\n",
    "# Use Ollama for local embeddings\n",
    "embedding_model = OllamaEmbeddings(model=\"mistral\")  # Change to \"deepseek\" if available\n",
    "\n",
    "# Store chunks in ChromaDB\n",
    "vector_db = Chroma.from_texts(text_chunks, embedding=embedding_model)\n",
    "\n",
    "print(f\"‚úÖ Indexed {len(text_chunks)} chunks in ChromaDB for document search.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
