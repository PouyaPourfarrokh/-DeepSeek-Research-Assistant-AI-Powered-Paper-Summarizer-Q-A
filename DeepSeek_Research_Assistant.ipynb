{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† DeepSeek Research Assistant üìÑüîç  \n",
    "### AI-Powered Research Paper Summarization & Q&A using DeepSeek-R1 & LangChain  \n",
    "This project allows **students & professors** to:  \n",
    "‚úÖ Upload a **research paper (PDF)**  \n",
    "‚úÖ Get an **AI-generated summary**  \n",
    "‚úÖ Receive **suggested questions** for better understanding  \n",
    "‚úÖ **Ask custom questions** for deeper insights  \n",
    "\n",
    "### ‚öôÔ∏è Tech Stack:  \n",
    "- **DeepSeek-R1-8B** (via Ollama) ‚Äì AI-powered text analysis  \n",
    "- **LangChain** ‚Äì Prompt engineering & AI interaction  \n",
    "- **ChromaDB** ‚Äì Vector database for semantic search  \n",
    "- **pdfminer.six** ‚Äì Extract text from PDFs  \n",
    "- **Streamlit** ‚Äì User-friendly UI (for deployment)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 2: Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Packages Imported Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4_/sz_9qvkn7cj9h353z48p55dw0000gn/T/ipykernel_62380/4015808447.py:11: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"huihui_ai/deepseek-r1-abliterated:8b\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pdfminer.high_level\n",
    "import langchain\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import streamlit as st\n",
    "\n",
    "# Initialize the DeepSeek model\n",
    "llm = Ollama(model=\"huihui_ai/deepseek-r1-abliterated:8b\")\n",
    "\n",
    "print(\"‚úÖ Packages Imported Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 3: Extract Text from PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted text from: /Users/pouyapourfarrokh/Desktop/AI&Data science Projects/DeepSeek Research Assistant/-DeepSeek-Research-Assistant-AI-Powered-Paper-Summarizer-Q-A/Research_papers/DeepSeek_V3.pdf\n",
      "DeepSeek-V3 Technical Report\n",
      "\n",
      "DeepSeek-AI\n",
      "\n",
      "research@deepseek.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total\n",
      "parameters with 37B activated for each token. To achieve efficient inference and cost-effective\n",
      "training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architec-\n",
      "tures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers\n",
      "an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training\n",
      "objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and\n",
      "high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to\n",
      "fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms\n",
      "other open-source models and achieves performance comparable to leading closed-source\n",
      "models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours\n",
      "for its full \n"
     ]
    }
   ],
   "source": [
    "pdf_directory = \"/Users/pouyapourfarrokh/Desktop/AI&Data science Projects/DeepSeek Research Assistant/-DeepSeek-Research-Assistant-AI-Powered-Paper-Summarizer-Q-A/Research_papers\"\n",
    "\n",
    "def get_latest_pdf(directory):\n",
    "    pdf_files = sorted(glob.glob(os.path.join(directory, \"*.pdf\")), key=os.path.getctime, reverse=True)\n",
    "    return pdf_files[0] if pdf_files else None\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return pdfminer.high_level.extract_text(pdf_path)\n",
    "\n",
    "latest_pdf = get_latest_pdf(pdf_directory)\n",
    "\n",
    "if latest_pdf:\n",
    "    extracted_text = extract_text_from_pdf(latest_pdf)\n",
    "    print(f\"‚úÖ Extracted text from: {latest_pdf}\")\n",
    "    print(extracted_text[:1000])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No PDFs found in the directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 4: Integrating LangChain for Summarization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4_/sz_9qvkn7cj9h353z48p55dw0000gn/T/ipykernel_62380/3182845835.py:10: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
      "/var/folders/4_/sz_9qvkn7cj9h353z48p55dw0000gn/T/ipykernel_62380/3182845835.py:14: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  return summary_chain.run(text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Research Paper Summary:\n",
      " <think>\n",
      "Okay, so I need to help summarize the research paper text for DeepSeek-V3 in clear and concise points. Let me read through the provided content carefully.\n",
      "\n",
      "The abstract mentions that DeepSeek-V3 is a strong Mixture-of-Experts (MoE) language model with 671B total parameters, specifically 37B activated per token. It uses Multi-head Latent Attention (MLA) and DeepSeekMoE architectures validated in DeepSeek-V2. They've introduced an auxiliary-loss-free strategy for load balancing and set a multi-token prediction training objective for better performance. The model is pre-trained on 14.8 trillion tokens, then goes through Supervised Fine-Tuning and Reinforcement Learning stages. Evaluations show it outperforms open-source models and matches closed-source ones, all while using only about 2.788M H800 GPU hours for training and being very stable without loss spikes.\n",
      "\n",
      "Looking at the sections: Introduction talks about the model's strengths; Architecture goes into details about MLA and their MoE structure with load balancing; Infrastructure covers compute clusters, training framework (DualPipe and Computation-Communication Overlap), FP8 training with mixed precision and quantization; Inference and Deployment mentions efficiency in inference.\n",
      "\n",
      "So I need to condense all this into key points without too much jargon. The user wants a clear summary, so focusing on the main innovations, performance highlights, training efficiency, architecture features, and evaluation results would be essential.\n",
      "</think>\n",
      "\n",
      "**Summary of DeepSeek-V3 Technical Report**\n",
      "\n",
      "1. **Model Overview**:  \n",
      "   - **Type**: Strong Mixture-of-Experts (MoE) language model.  \n",
      "   - **Parameters**: 671B total parameters, with 37B activated per token.  \n",
      "   - **Architecture Features**: Utilizes Multi-head Latent Attention (MLA) and DeepSeekMoE architecture, which includes an auxiliary-loss-free load balancing strategy and multi-token prediction training objective for improved performance.\n",
      "\n",
      "2. **Training Process**:  \n",
      "   - **Pre-training**: Pre-trained on 14.8 trillion diverse and high-quality tokens.  \n",
      "   - **Fine-tuning & Reinforcement Learning (RL)**: Followed by Supervised Fine-Tuning and RL stages to enhance capabilities.\n",
      "\n",
      "3. **Performance and Evaluation**:  \n",
      "   - **Comparisons**: Outperforms open-source models and matches leading closed-source models in performance.  \n",
      "   - **Training Efficiency**: Requires only 2.788M H800 GPU hours for full training, with a stable process that avoids loss spikes or rollbacks.  \n",
      "\n",
      "4. **Innovation and Features**:  \n",
      "   - **Load Balancing Strategy**: Auxiliary-loss-free method for efficient load balancing.  \n",
      "   - **Training Objective**: Multi-token prediction objective to enhance model performance.  \n",
      "\n",
      "5. **Infrastructure and Implementation**:  \n",
      "   - **Compute Clusters and Training Framework**: Uses DualPipe with computation-communication overlap for efficiency, and employs FP8 training with mixed precision, quantization, and low-precision storage to optimize performance.\n",
      "\n",
      "6. **Inference and Deployment**:  \n",
      "   - The model is designed for efficient inference and deployment, with checkpoints available on GitHub.\n"
     ]
    }
   ],
   "source": [
    "# Initialize LangChain with DeepSeek model\n",
    "llm = Ollama(model=\"huihui_ai/deepseek-r1-abliterated:8b\")\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"\"\"Summarize the following research paper text in clear and concise points:\\n\\n{text}\\n\\n### Summary:\"\"\"\n",
    ")\n",
    "\n",
    "# LangChain Summarization Chain\n",
    "summary_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
    "\n",
    "def summarize_text(text):\n",
    "    text = text[:3000]  # Limit to model's token capacity\n",
    "    return summary_chain.run(text)\n",
    "\n",
    "# Generate summary\n",
    "summary = summarize_text(extracted_text)\n",
    "print(\"üìå Research Paper Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìÇ Cell 5: LangChain for Generating Suggested Questions & Answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå 5 AI-Generated Questions & Answers:\n",
      "\n",
      "<think>\n",
      "Alright, let's dive into the thought process of generating these questions and answers based on the provided research paper summary of DeepSeek-V3.\n",
      "\n",
      "First, I need to understand the key points covered in the summary. The model is a Mixture-of-Experts (MoE) language model with significant parameters, utilizing specific architectural innovations like Multi-head Latent Attention (MLA). It's pre-trained on an enormous amount of data and goes through fine-tuning and reinforcement learning stages, achieving impressive performance metrics while being computationally efficient.\n",
      "\n",
      "To create thought-provoking questions, I should focus on areas that highlight the model's strengths, unique features, and implications. The answers need to be concise yet informative, addressing why these aspects matter in the context of language models.\n",
      "\n",
      "1. **Why is the choice of 671B parameters significant for DeepSeek-V3?**  \n",
      "   - The total number of parameters determines the model's capacity. With 671B, it can handle complex tasks and retain more information, which is crucial for its performance levels.\n",
      "\n",
      "2. **How does Multi-head Latent Attention (MLA) improve upon traditional attention mechanisms in DeepSeek-V3?**  \n",
      "   - MLA allows for better context handling by storing latent representations across different heads, enhancing the model's ability to process diverse inputs efficiently.\n",
      "\n",
      "3. **What is the significance of the auxiliary-loss-free load balancing strategy in DeepSeek-V3?**  \n",
      "   - This strategy optimizes resource allocation, preventing bottlenecks and ensuring efficient training across all workers without relying on additional losses, thus enhancing overall performance.\n",
      "\n",
      "4. **How does the multi-token prediction training objective contribute to improved model performance?**  \n",
      "   - By focusing on predicting multiple tokens at once, it forces the model to capture longer-range dependencies and better contextual understanding, leading to more accurate predictions.\n",
      "\n",
      "5. **What makes DeepSeek-V3's training efficiency stand out compared to other models?**  \n",
      "   - The use of DualPipe with computation-communication overlap reduces redundant computations, allowing faster training while minimizing resource waste.\n",
      "\n",
      "These questions and answers not only highlight the technical innovations but also their practical implications for developing and deploying large language models.\n"
     ]
    }
   ],
   "source": [
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"summary\"],\n",
    "    template=\"\"\"Based on the following research paper summary, generate 5 thought-provoking questions and their corresponding answers:\\n\\n### Summary:\\n{summary}\\n\\n### Questions & Answers:\"\"\"\n",
    ")\n",
    "\n",
    "qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
    "\n",
    "def generate_questions_and_answers(summary):\n",
    "    return qa_chain.run(summary)\n",
    "\n",
    "# Generate 5 questions and their answers\n",
    "questions_answers = generate_questions_and_answers(summary)\n",
    "\n",
    "print(\"üìå 5 AI-Generated Questions & Answers:\\n\")\n",
    "print(questions_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
